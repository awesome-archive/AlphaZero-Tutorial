{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Alpha Zero Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the preceeding parts, we have explained some of the key components of the Alpha Zero algorithm. But the way we did it was a bit disconnecting and we need to connect those parts to see how Alpha Zero works. Let's breifly overview the major components and then discuss them together in an algorithm.\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "In the Residual Network Notebook, we showed what makes ResNet different than a vanilla Convolutional Neural Network. We discussed ResNet Architecture in particular, because it is used at the core of the AlphaZero algorithm. The neural network $f_{\\theta}$ is parametrized by set of weights $\\theta$. It takes as input the board representation of the game, $s$ and outputs two things: a continuous value $v_{\\theta}(s)$ in the range $[-1,1]$ from the perspective of the current player and a policy $p_{\\theta}(s)$ that is a probability vector over all the possible states. \n",
    "\n",
    "Let's see the Neural Network architecture with respect to game Go that is used in AlphaGo Zero. \n",
    "\n",
    "### Input:\n",
    "> - Size of the input = 19 x 19 x 17\n",
    "> - 19 x 19 is the 2D board size with height and width = 19\n",
    "> - 17 is the depth of the input, which corresponds to possible game states. Each layer of these 17 layers is a binary feature plane. First 8 binary feature planes $(X_{i})$ indicate the presence of current player's stones. Next 8 binary feature planes $(Y_{i})$ indicate the presence of opponent's stones. The last feature plane $C$ states whose turn it is currently. \n",
    "> - So input features = $[X_{i}, Y_{i}, X_{i-1}, Y_{i-1}, ... , X_{i-7}, Y_{i-7}, C]$\n",
    "\n",
    "The input is then fed into a __Residual Tower__. A residual tower consists of:\n",
    "> - a single convolution block\n",
    "> - either 19 or 39 residual blocks\n",
    "\n",
    "Let's discuss each of these two components of the residual tower.\n",
    "\n",
    "### Residual Block:\n",
    "\n",
    "#### Convolution Block Operations:\n",
    "> - It is a convolution block of 256 filters of kernel size 3 x 3 with stride 1\n",
    "> - Batch Normalization is then used\n",
    "> - Then a rectified non-linearity (such as ReLU) is applied\n",
    "\n",
    "#### Residual Block Operations:\n",
    "Following operations happen sequentially. Output of each is fed as input to the succeeding layer. \n",
    "> - a convolution of 256 filters of kernel size 3 x 3 with stride 1\n",
    "> - Batch Normalization applied\n",
    "> - Rectified Non-Linearity applied\n",
    "> - a convolution of 256 filters of kernel size 3 x 3 with stride 1\n",
    "> - Batch Normalization applied\n",
    "> - A __skip connection__ that adds input to the block\n",
    "> - Rectified Non-Linearity applied\n",
    "\n",
    "The output of the residual tower goes into two __heads__: __policy head and value head__.\n",
    "\n",
    "Let's now look at the operations of these two heads.\n",
    "\n",
    "### Policy Head:\n",
    "\n",
    "> - a convolution of 2 filters of kernel size 1 x 1 with stride 1\n",
    "> - Batch Normalization applied\n",
    "> - Rectified Non-Linearity\n",
    "> - a fully connected linear layer that outputs a vector of size (# of moves)\n",
    "\n",
    "### Value Head:\n",
    "\n",
    "> - a convolution of 1 filter of kernel of size 1 x 1 with stride 1\n",
    "> - Batch Normalization applied\n",
    "> - Rectified Non-Linearity\n",
    "> - a fully connected linear layer to a hidden layer of size 256\n",
    "> - Rectified Non-Linearity\n",
    "> - a fully connected linear layer to a scalar\n",
    "> - a __tanh__ non-linearity outputting a scalar in range [-1,1]\n",
    "\n",
    "A high level view of the AlphaZero Neural Network Architecture is as follows:\n",
    "\n",
    "Input --> Residual Tower (containing Convolution Block and Residual Blocks) --> Policy Head and Value Head\n",
    "\n",
    "<img src=\"images/alphneur.PNG\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Let's now discuss the training process of this Neural Network.\n",
    "\n",
    "### Training:\n",
    "\n",
    "At the end of each game of self-play, the neural network is provided training examples of the form $(s_{t}, \\pi_{t}, z_{t})$. Here $\\pi_{t}$ is an estimate of the policy from $s_{t}$ and $z_{t}$ is the final outcome of the self-play game from the perspective of the current player at $s_{t}$. We will soon see how to obtain $\\pi_{t}$. So essentially, a neural network outputs $p_{\\theta}(s_{t})$ and $v_{\\theta}(s_{t})$, it gets a training example for $s_{t}$ as $\\pi_{t}$ and $z_{t}$. So to make $p_{\\theta}(s_{t})$ and $v_{\\theta}(s_{t})$ close to $\\pi_{t}$ and $z_{t}$, we minmize the following loss function:\n",
    "\n",
    "<img src=\"images/alphloss.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The main idea is that over time, the neural network will learn what states eventually lead to wins or losses Learning the policy gives a good estimate of what the best action is from a given state. \n",
    "\n",
    "Now, we haven't really seen how to obtain $\\pi_{t}$ and to see that we now reintroduce the algorithm we discussed in the preceeding notebook -- __Monte Carlo Tree Search__. Essentially, MCTS is used for policy (from the Neural Network) improvement. Let's now see how MCTS works together with the Neural Network.\n",
    "\n",
    "## Monte-Carlo Tree Search for Policy Improvement\n",
    "\n",
    "Remember, in a game search tree, each node represents a game state (board configuration). A directed edge exists between two nodes $i -> j$ if a valid action can cause a transition from $i$ to $j$. We start with an empty search tree with a root node representing the empty-board state. We then expand the search tree one node at a time. When we encounter a new node, we normally perform a rollout. But we discussed that rollout is a time-consuming process for games like Go. So in AlphaZero, instead of performing a rollout, the value of the new node is obtained from the neural network itself. The value is the propagated up the trajectory to update the values. \n",
    "\n",
    "Notice that the improvement on traditional MCTS is the use of Neural Network to obtain the value of the new node instead of performing a rollout. This step is faster and also as the neural network learns, the prediction will be more accurate than a random rollout. \n",
    "\n",
    "Let's discuss this improved Monte-Carlo Tree Search in more detail. As we had discussed previously, we maintain the following attributes for the tree search:\n",
    "> - $Q(s, a)$: expected value of taking action $a$ from state $s$\n",
    "> - $N(s, a)$: number of times we took action $a$ from state $s$ across simulations\n",
    "> - $P(s, .)$: this is $p_{\\theta}(s)$, the initial estimate of taking an action from state $s$ according to the policy returned by the current neural network. \n",
    "\n",
    "Remember, we use an Upper Confidence Bound to decide which node to select during tree expansion. Here, let us present a bit modified version of the UCB, however it doesn't make much difference in terms of exploitation vs exploration. \n",
    "\n",
    "> - $U(s, a) = Q(s, a) + c . P(s, a) . \\frac{\\sqrt{\\sum_{b}N(s, b)}}{1 + N(s, a)}$\n",
    "\n",
    "This is how MCTS works to improve the initial policy output by the neural network. We initialize our empty game search tree with $s$ as the root. A single MCTS simulation proceeds as follows:\n",
    "> - Compute action $a$ that maximizes the UCB $U(s, a)$\n",
    "> - If the next state $s'$ exists in our tree, we recursively call the search on $s'$. \n",
    "> - If it does not exist, we add the new state to our tree and initialize $P(s', .) = p_{\\theta}(s')$ and the value $v(s') = v_{theta}(s')$ from the neural network (instead of performing a rollout) and initialize $Q(s', a)$ and $N(s', a)$ to 0 for all $a$.\n",
    "> - Propagate the value $v(s')$ up along the search path seen in the current simulation and update all $Q(s, a)$ values.\n",
    "> - If we encounter a terminal state, we propagate the actual value (win/draw/loss).\n",
    "\n",
    "We perform multiple MCTS simulations (usually a fixed number or time) to expand our game search tree. After doing so, the $N(s, a)$ values at the root provide a better approximation for the policy. The improved stochastic policy, $\\pi_{t}$ is simply $\\frac{N(s, .)}{\\sum_{b}N(s, b)}$. During self-play, we perform MCTS and pick a move by sampling a move from the improved policy $\\pi_{t}$. This is how we obtain $\\pi_{t}$ that is used to train the neural network to improve $p_{\\theta}(s_{t})$.\n",
    "\n",
    "<img src=\"images/nnmcts.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def search(s, game, NN):\n",
    "    if game.gameEnded(s):\n",
    "        return -game.gameReward(s)\n",
    "\n",
    "    if s not in visited:\n",
    "        visited.add(s)\n",
    "        P[s], v = NN.predict(s)\n",
    "        return -v\n",
    "  \n",
    "    max_ucb, best_action = -float(\"inf\"), -1\n",
    "    for a in range(game.ValidActions(s)):\n",
    "        u = Q[s][a] + c*P[s][a]*np.sqrt(sum(N[s]))/(1+N[s][a])\n",
    "        if u > max_ucb:\n",
    "            max_ucb = u\n",
    "            best_action = a\n",
    "    a = best_action\n",
    "    \n",
    "    s_next = game.nextState(s, a)\n",
    "    v = search(s_next, game, NN)\n",
    "\n",
    "    Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a]+1)\n",
    "    N[s][a] += 1\n",
    "    return -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "We have shown how the Neural Network connects with the Monte-Carlo Tree Search algorithm and how it is trained using examples obtained from Monte-Carlo Tree Search. The Alpha Zero algorithm is essentially a Policy Iteration algorithm that improves via self-play. Let's see how this algorithm works on a higher level.\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "Here is the complete algorithm:\n",
    "\n",
    "> - Initialize the Neural Network, $f_{\\theta}$, with random weights $\\theta_{0}$, which means we start with a random policy and value network and hence the network won't be able to predict aything intelligent in the beginning. \n",
    "> - In each iteration of this algorithm, play a fixed number of self-play games.\n",
    ">> - In each game of self-play:\n",
    ">>> - At each time step t (at each turn of the self-play game), we peform a fixed number of MCTS simulations (based on $f_{\\theta_{i-1}}$) from the current state $s_{t}$. After that many simulations, we have a policy estimate at the root node. We then pick a move at time t by sampling from the improved policy $\\pi_{t}$.\n",
    ">>> - We now have a training example $(s_{t}, \\pi_{t}, z_{t})$, where $z_{t}$ is the reward achieved at the last time step of the game of self-play. \n",
    ">>> - Since the data for each time step t is stored as $(s_{t}, \\pi_{t}, z_{t})$, at the end of the game, we have a list containing data for each time step in that game. This list serves as the training data.\n",
    ">> - After a fixed number of games of self-play, we have the data we obtained after evey single game of self-play.\n",
    "> - At the end of each iteration, the neural network is trained with the obtained training examples. The old network, $f_{\\theta_{i-1}}$ is then put againt the new (trained) network, $f_{\\theta_{i}}$. If the new network wins more than a set threshold fraction of games (55% in the DeepMind paper), the network is updated to the new network, otherwise we continue another iterations to obtain more training examples.\n",
    "\n",
    "\n",
    "This is it. This is how we train our neural network and it improves after every iteration. So after every iteration, it becomes a better player. Here is the high-level implementation of policy iteration Alpha Zero algorithm::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def policy_iteration(game):\n",
    "    NN = init_NN()\n",
    "    training_examples = []\n",
    "    for i in range(noIter):\n",
    "        for e in range(noGames):\n",
    "            examples += playGame(game, NN)\n",
    "        new_NN = train_NN(examples)\n",
    "        wins = play(new_NN, NN)\n",
    "        if wins > threshold:\n",
    "            NN = new_NN\n",
    "    return NN\n",
    "\n",
    "def playGame(game, NN):\n",
    "    examples = []\n",
    "    s = game.startState()\n",
    "    mcts = MCTS()\n",
    "    \n",
    "    while True:\n",
    "        for j in range(noMCTSSimulation):\n",
    "            mcts.search(s, game, NN)\n",
    "        examples.append([s, mcts.pi(s), None])\n",
    "        a = random.choice(len(mcts.pi(s)), p=mcts.pi(s))\n",
    "        s = game.nextState(s, a)\n",
    "        if game.gameEnded(s):\n",
    "            examples = reward(examples, game.gamereward(s))\n",
    "            return examples"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
